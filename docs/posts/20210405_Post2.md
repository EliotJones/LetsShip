# Lets Ship: Architecture

This is the 2nd post in the Lets Ship series. Post 1 covered the introduction and setting up a very simple project structure. In this post we'll cover the high level application design.

C# has a reputation for being needlessly verbose and enterprise-y. This codebase does very little to disprove that.

In retrospect I would have done away with the `PriceFalcon.Domain` project entirely and done data access directly in the request handlers and crawler code, rather than inside the `PriceFalcon.Infrastructure project`.

With that said lets look at the architecture by starting with the use-case.

The basic happy-path flow of the finished application is as follows:

1. A user enters their email and verifies the email by means of a token emailed to them, the token is then revoked.
2. The verified user enters the URL of the website they wish to monitor.
3. The status page displays the progress of the initial crawl.
4. Once the initial crawl is complete the user selects the price element they wish to monitor and submits the form.
5. The crawler occassionally polls the selected element on the web page to look for price changes.

## M̶i̶c̶r̶o̶services

Since a lot of websites these days rely heavily on JavaScript even for displaying a fairly static product page we're going to need something more than `curl` (or `HttpClient`) to do the crawling and monitoring part.

[Selenium](https://www.selenium.dev/) matches this requirement well, another option would be [Puppeteer](https://developers.google.com/web/tools/puppeteer/), both have C# bindings but I'm more familiar with Selenium so I'll be using that.

Since Selenium will need us to launch and manage instances of a headless browser it makes sense to split the crawl and monitor responsibility out from the web app for the following reasons:

- Crashes in the crawler won't impact the web application.
- Security exploits in the crawler won't impact the web application. Since the crawler involves running a browser against random URLs it is more vulnerable than the web application. Ideally, in future, the crawler would use a much more restricted database user for its connection, but for now we want to ship something.
- The crawler can scale independently of the web application. Since the resource demanding part of the process is running a browser instance the crawler is going to require more resources than the web part, by splitting them up the crawler can be auto-scaled separately.

This sounds a lot like microservices, but I'd propose an alternative name of "services". I think a lot of microservices hype is overcooked and in general I'd prefer a single monolith, but I think separating 2 non-trivial, self-contained services makes sense here.

We could have used a message queue to mediate communication between the crawler and web app after the user submits the job to run, but we have a perfectly usable database with support for row-level locking and for now a message queue seems like pointless complexity and operations overhead.

Since we're relying heavily on the database in future we might need to add an instance of [PgBouncer](https://www.pgbouncer.org/features.html) for scale since postgres connections have quite heavyweight memory demands. But keep in mind, we just want to ship something, let the fail whale appear first and solve later, it worked for Twitter.

## Project structure

We already have our MVC web application which will be how the user interacts with our app.

We need some of the crawler logic to be shared between the web and crawler app, because when a user selects an element to monitor from the crawled page in the web app we need to check the crawler can uniquely identify it for future job runs.

We also need a process to host the crawler and poll for jobs.

Finally we want to decouple our MVC app from the bulk of our logic because we might switch to a React SPA talking to a Web API in future. We want to keep our user facing application as thin as possible to make pivoting on these decisions easier. For this purpose and also because I have yet to find a superior approach for organising application logic we'll be using [MediatR](https://github.com/jbogard/MediatR/wiki).

The amount of boilerplate required by MediatR seems onerous at first. But in my experience MediatR is a great way to scale an application from 1,000 to 100,000 lines of code without having to perform large refactorings or rearchitecting.

The number one benefit of the MediatR/CQRS approach is the mind-set it encourages. By modelling the domain as simple event/message data objects (commands/queries) that are dispatched and handled without the callers involvement the pattern guides the developer towards a way of modelling the domain in terms of the events that occur.

This results in a more natural way of organising code than arbitrary service/repository layered design. A developer knows, for instance, that when a user requests a sign up, the code for that will be in the `RequestSignUpHandler<>`. Inside this handler the developer can choose the most natural way to handle this request without having to contort code to match one-size-fits-all architecture imposed from above. In addition they are encouraged to consider all the side-effects or notifications that might need to be raised as a result of that request and they have a centralized place to handle them.

One example of an architecture based on MediatR is the [vertical slice architecture](https://headspring.com/2019/11/05/why-vertical-slice-architecture-is-better/).

On the other hand to take another heavily used C# library, using AutoMapper is usually a sign that the code is becoming enterprise-poisoned. Every abstraction imposes a cost and the job of a developer is to minimise that cost. We should strive to write code that is easy to delete, above all else.

In the end I accidentally fell into the pattern of using services and repositories on the back-end of my request handlers which was a pointless abstraction. However refactoring to remove that would be trivial and would not affect the web application code that calls those events.

The final project structure was:

- PriceFalcon.App - the commands, queries and their handlers.
- PriceFalcon.Crawler - the logic for using Selenium to load pages and select the price element from them.
- PriceFalcon.Domain - the data types shared between all other projects, such as draft jobs, jobs and users. This project name doesn't make much sense because it's an anemic domain so it could have probably been PriceFalcon.Core and just contained the `JobStatus` enums.
- PriceFalcon.Infrastructure - everything that "does something" externally plus the config. Examples of "doing something" are interaction with the database, sending emails, etc. It only exposes interfaces publicly so we could swap out SendGrid for another email sending service easily. Data access also ended up in here because I'm corrupted by the old C# school of thought that believed people just randomly swapped their databases willy-nilly. Going back to the rant from part 1, your application code is more likely to be killed off before the database, this was a pointless abstraction.
- PriceFalcon.JobRunner - A slim shell to invoke the crawler. Contains the logic to acquire and manage job locks, poll the database and find the right drivers for Selenium based on the host OS.
- PriceFalcon.Web - the UI for driving the user facing part of the application. The project we created in step 1.

## Data access and schema management

I've tried Entity Framework a few times in a few places and never managed to get on with it. In order to keep data access simple and reduce the amount of magic I prefer using Dapper and [Dapper.Contrib](https://www.nuget.org/packages/Dapper.Contrib/) to use convenience wrappers over ADO.NET and [Npgsql](https://www.nuget.org/packages/Npgsql).

Unfortunately Dapper.Contrib doesn't really have nice support for conventional postgres table and column names still so I had to write a little [helper method for inserting entities](https://github.com/EliotJones/LetsShip/blob/main/src/PriceFalcon.Infrastructure/DataAccess/DapperPostgresHelper.cs). However Dapper.Contrib has support for mapping results of a query using conventional postgres names by enabling [MatchNamesWithUnderscores](https://github.com/EliotJones/LetsShip/blob/main/src/PriceFalcon.Infrastructure/DataAccess/IConnectionProvider.cs#L26).

Not using Entity Framework means that we need to handle migrations ourselves. I was recently introduced to [Evolve](https://evolve-db.netlify.app/) for running migrations in process. It handles running migration scripts, recording the schema version in the database and locking the database when running multiple servers. The code to invoke the migrations [is here](https://github.com/EliotJones/LetsShip/blob/main/src/PriceFalcon.Infrastructure/Bootstrapper.cs#L29) and is called at startup of the web application.
