# Lets Ship: Docker and GitHub actions

This is the 3rd post in the Lets Ship series. Post 1 covered the introduction and setting up a very simple project structure. Post 2 covered the high level design and architecture. In this post we'll cover containerising our application and building using GitHub actions.

## Docker for deployment

By packaging our application into a Docker container we can ensure the build artifact has all necessary dependencies, system libraries and services necessary to run in production. We uncouple our application from a specific server and produce a reproducible packaged application that can run on any container-enabled service or server.

With .NET Core (now .NET 5) containerization of .NET apps has become extremely easy. Microsoft provide [a set of base images](https://hub.docker.com/_/microsoft-dotnet) for .NET apps that provide a working .NET environment out-of-the-box.

For our finished application we need a container for our MVC web application and one for our Selenium crawler application.

We use Dockerfiles to specify how to build a Docker image that can be used to start one or more containers. Each command in a Dockerfile creates a layer in the resulting image. Layers build on top of each other to produce the final image. For example you might have the following layers:

- Use Ubuntu 20.04
- Install Python
- Create a directory
- etc...

There's a [tutorial here](https://docs.docker.com/engine/examples/dotnetcore/) on creating Dockerfiles for .NET applications. Our resulting Dockerfile is quite similar. Let's go through it step by step:

```
FROM mcr.microsoft.com/dotnet/sdk:5.0-alpine3.13 as build

WORKDIR /build

COPY src .

# linux-musl-x64 is for lightweight containers like alpine linux.
RUN dotnet restore -v n -r linux-musl-x64 PriceFalcon.Web/PriceFalcon.Web.csproj

# Run the publish targeting the same runtime but with no restore.
RUN dotnet publish -v n --no-restore -c Release -r linux-musl-x64 -o web PriceFalcon.Web/PriceFalcon.Web.csproj

FROM mcr.microsoft.com/dotnet/runtime-deps:5.0.4-alpine3.13 as run

# Create a user called "app" who has ownership of the "app" directory.
RUN adduser --disabled-password --home /app --gecos '' app && chown -R app /app

USER app

WORKDIR /app

COPY --from=build /build/web .

ENV ASPNETCORE_URLS=http://+:6110 \
    SiteUrl=http://localhost:6110

EXPOSE 6110

ENTRYPOINT [ "./PriceFalcon.Web" ]
```

(Sidebar: I'm going to use **directory** and **folder** interchangeably in the walkthrough. I'm used to thinking in terms of folders but directory is a more 'proper' term, but I don't think it really matters.)

Firstly we define a base image to start building the final image from. We use the predefined Microsoft provided image:

```
FROM mcr.microsoft.com/dotnet/sdk:5.0-alpine3.13 as build
```

This tells Docker we're using an Alpine Linux 3.13 image with the .NET 5.0 SDK installed on it.

The .NET SDK includes the build tools necessary to build and publish our application, e.g. `dotnet build` and `dotnet publish`.

Alpine Linux is a lightweight operating system built on the Linux kernel. By using Alpine Linux we can reduce the size of the resulting image.

Reducing image size is important where we're charged for inbound/outbound traffic since transferring large Dockerfiles could quickly increase our bill. Many private Docker registries will charge for traffic to and from the registry so reducing the image size saves money.

#### Multi-stage builds

The `FROM <image_name>` line ends with `as build`. This enables [multi-stage builds](https://docs.docker.com/develop/develop-images/multistage-build/). Multi-stage builds split the build of the final image into separate stages.

Each stage can use a different base image. In this case we're splitting the build of the application from the final image used to run it.

Why is having more than one base image useful? For .NET projects the .NET SDK includes many tools we don't actually need to run the final application. In multi-stage builds only the final stage is shipped with the image. By leaving the SDK build tools in the first of the 2 stages we don't need to ship the full SDK in the final image. Yet again this reduces the image size saving time and money.

The stage `build` can be called with anything. It's just a label used to refer to the stage later on in other stages, such as the `COPY --from=<stage_name>` command.

Next up we define the working directory inside our current stage to store the raw (not compiled) source-code in:

```
WORKDIR /build
```

This tells Docker the next commands will run inside a folder called `build`. The naming of this folder is arbitrary and unrelated to the name of the stage.

To get the source code from our machine into the docker build we use the docker `COPY` command.

```
COPY src .
```

This copies all files and folders from the `src` folder of our repository into the current working directory (`.` which was defined as `/build` in the previous step) inside our Docker container.

The build command must be run from the root folder of the repository on the host machine. Docker requires the files to copy to be in the working directory or a subdirectory. This means because the Dockerfile is in the `docker/web` folder relative to the root of the directory we must run `docker build` from the root directory and give it the full path to the Dockerfile using the `-f` argument, e.g:

```
docker build -t MyOutputName -f docker/web/Dockerfile .
```

(Sidebar: there are 2 ways to get files and folders from outside the Docker image into it, `COPY` and `ADD`. [This question](https://stackoverflow.com/questions/24958140/what-is-the-difference-between-the-copy-and-add-commands-in-a-dockerfile) goes into some more details on the difference but the consensus seems to be stick with `COPY`).

#### dotnet builds in Docker

Now we have our source code inside our Docker container and the .NET 5 SDK included from the base image. The next step is to build and publish the code:

```
RUN dotnet restore -v n -r linux-musl-x64 PriceFalcon.Web/PriceFalcon.Web.csproj
```

A useful optimization that Docker provides is that it will cache layers (each line in the Dockerfile) when building an image.

Caching means that if the layer is unchanged it will use the existing layer when re-building, making the build quicker. For this reason it is useful to split the `restore` and `publish` steps of the build using `dotnet`.

Here we run `dotnet restore` inside the `build` folder. Since we copied from `src` to `build` in the earlier step the contents of the `build` folder are identical to the `src` folder of our repository, therefore the specified path to restore `PriceFalcon.Web/PriceFalcon.Web.csproj` is relative to the `src` folder.

(Sidebar: The `-v n` flag sets verbosity of the logging to `normal` rather than the default `quiet`. I added this because my laptop seemed to have a problem where `dotnet restore` inside a Dockerfile build would hang forever until the build was cancelled and the Docker service was restarted so the logging could help me see if the restore was still running or broken.)

To reduce image size further we're using self-contained builds in .NET. The `-r linux-musl-x64` option tells `dotnet restore` to restore for the runtime (`-r`) `linux-musl-x64`. This targets Linux distributions that use the `musl` version of `libc`, this includes Alpine Linux, the base image we are using. Read more about runtime identifiers [here](https://docs.microsoft.com/en-us/dotnet/core/rid-catalog).

After restore has completed we need to publish the application:

```
RUN dotnet publish -v n --no-restore -c Release -r linux-musl-x64 -o web PriceFalcon.Web/PriceFalcon.Web.csproj
```

We target the Alpine Linux specific runtime using `-r linux-musl-x64`.

We also specify `--no-restore` so that `dotnet` doesn't duplicate the restore we just ran in the previous line/layer. In addition we specify the published build should use the `Release` configuration, rather than `Debug` (`-c Release`). Finally we specify the output folder relative to our current directory (`build`) using `-o web`. This means the published application code will be in the `build/web` folder on the build stage.

We now have our published application and the work for the build stage of our multi-stage build is complete.

#### Running dotnet applications in Docker

Next up we define the runtime environment for our image:

```
FROM mcr.microsoft.com/dotnet/runtime-deps:5.0.4-alpine3.13 as run
```

We're using Alpine Linux 3.13 from the official Microsoft base images like the build stage. But here we're using the `runtime-deps` image. The runtime dependencies image has this description:

> This image contains the native dependencies needed by .NET. It does not include .NET. It is for self-contained applications.

This includes things like various C libraries needed to run .NET applications on the specific Linux distribution but not .NET itself which we included with the compiled application during our self-contained publish step.

This stage is using the alias `run`.

Because it's [a bad idea](https://americanexpress.io/do-not-run-dockerized-applications-as-root/) to run things as the default Linux user `root` even inside containers we create a new user called `app` with the home directory called `/app` to run our web application as:

```
RUN adduser --disabled-password --home /app --gecos '' app && chown -R app /app
```

We also give ownership of the `/app` directory to our `app` user using the change owner (`chown`) command: `chown -R app /app`. We then switch the following commands to run as our newly created user and set the working directory to their home directory `/app`:

```
USER app

WORKDIR /app
```

Then we copy the published artifacts from our previous build stage into the current app folder:

```
COPY --from=build /build/web .
```

The `--from` parameter specifies that we're copying from the `/build/web` folder of a build stage called `build` rather than from the host machine, into the current folder `.` of our image.

Next we define a couple of environment variables. The first is used by ASP.NET to select the port to listen on for HTTP traffic. The other is defined in my code as the URL for the site when sending emails:

```
ENV ASPNETCORE_URLS=http://+:6110 \
    SiteUrl=http://localhost:6110

EXPOSE 6110
```

The `EXPOSE` command tells Docker to expose port `6110` of the container, this is the same port we told ASP.NET to listen on using `ASPNETCORE_URLS=http://+:6110`. Because we're running as a non-root user we have to run on a port [greater than 1024](https://github.com/dotnet/aspnetcore/issues/4699#issuecomment-432756438) in order for Kestrel to run successfully, otherwise we will get a `System.Net.Sockets.SocketException (13): Permission denied` exception.

Finally we're ready to run the application. To do this we define the entrypoint. The entrypoint is the "thing that Docker will run when the image is started". Because we published a self-contained app this is the `PriceFalcon.Web` file produced for the Linux build:

```
ENTRYPOINT [ "./PriceFalcon.Web" ]
```

When we run a container for this image it will start the web application listening on port 6110.

#### Docker for the crawler

We're not going to go into this Dockerfile in such excrutiating detail, much of it is the same as the web Dockerfile:

```
FROM mcr.microsoft.com/dotnet/sdk:5.0 as build

RUN mkdir build

WORKDIR /build

COPY src .

RUN mkdir crawler

RUN dotnet restore -v n -r linux-x64 PriceFalcon.JobRunner/PriceFalcon.JobRunner.csproj

RUN dotnet publish -v n --no-restore -c Release -r linux-x64 -o crawler PriceFalcon.JobRunner/PriceFalcon.JobRunner.csproj

FROM mcr.microsoft.com/dotnet/runtime-deps:5.0.4-focal as run

RUN apt-get update && \
  apt-get -y --no-install-recommends install \
  xvfb \
  firefox \
  libfontconfig \
  libfreetype6 \
  xfonts-cyrillic \
  xfonts-scalable \
  && rm -rf /var/lib/apt/lists/* \
  && apt-get -qyy clean

# For Xvfb (X virtual frame buffer, enables in-memory GUI without GUI installed).
ENV DISPLAY :99

# From https://github.com/SeleniumHQ/docker-selenium/blob/trunk/NodeBase/Dockerfile
ENV LANG_WHICH en
ENV LANG_WHERE US
ENV ENCODING UTF-8
ENV LANGUAGE ${LANG_WHICH}_${LANG_WHERE}.${ENCODING}
ENV LANG ${LANGUAGE}

RUN mkdir app

WORKDIR /app

COPY docker/crawler/run-via-xvfb.sh /app/run-via-xvfb.sh
RUN chmod a+x /app/run-via-xvfb.sh

COPY --from=build /build/crawler .

RUN chmod a+x Drivers/Linux/geckodriver

ENTRYPOINT [ "./run-via-xvfb.sh" ]
```

One key difference here is we're running on the `5.0-focal` and `5.0.4-focal` (Ubuntu) base images for the build and run stages respectively. This is because the crawler requires an installation of Firefox to run. It is probably possible to do this on Alpine Linux but more complex, so we use an image that supports `apt-get install` for Firefox.

The steps of the build stage are almost identical except the name and location of the project being built.

The run stage is more complex however much of the content is adapted from the [official Selenium docker image](https://github.com/SeleniumHQ/docker-selenium/blob/trunk/NodeBase/Dockerfile). We first install Firefox, some supporting font-related libraries as well as Xvfb, the X Virtual Frame Buffer which enables an in-memory GUI for rendering the browser.

In order to use Xvfb we need to launch our crawler program in a display context. To do this we use a separate Bash script which first starts Xvfb with display number 99, then we run our program in the same command. This is why our entrypoint is a script that we copy from our repository instead of the application directly. This isn't quite how the official Selenium Docker image does it, but it was my approach and it works enough for to ship the MVP, so I'm ignoring the terribleness of it.

Also worth noting is I didn't add the non-root user here, I absolutely should have since this container interacts with a browser so is worth securing well.

#### Build and run

With the Dockerfiles complete we can build and run them to test locally. For web (from the root of the repository):

```
docker build -t falconweb -f docker/web/Dockerfile .
```

And for the crawler:

```
docker build -t falconcrawl -f docker/crawler/Dockerfile
```

When the build is complete we can run `docker image ls` to check the images are present locally, their name, ID and size.

To start a container from the image and connect to it using a shell, rather than starting the application immediately we can run (on Windows/Mac; the use of `host.docker.internal` in the connection string does not work the same way on Linux):

```
# docker run --rm -it -p 6110:6110 -e ConnectionStrings__Default="Server=host.docker.internal;Port=5432;Database=pricefalcon;User Id=devwrite;Password=xE:UZj4buVy2&&3n;" -e ASPNETCORE_ENVIRONMENT="Development" -e PRICE_FALCON_SENDGRID="SG.fakeapikey" --entrypoint /bin/ash falconweb
```

There are quite a few options here, let's break them down:

- `docker run` - The command to run.
- `--rm` - The container should be automatically cleared up when it stops running.
- `-it` - Requests the container to start interactively and allow us to connect through a shell.
- `-p` - Provides the mapping between the port on our host machine and port exposed by Docker using `-p 6110:6110` which maps port 6110 on our local machine to port 6110 of the Docker container.
- `-e` - We provide the values of several environment variables used by our application using `-e KEY="VALUE"`.
- `--entrypoint` - Override the default entrypoint (our web application) to instead start a shell instance (`/bin/ash` on Alpine Linux).
- `falconweb` - the name of the image to run.

When you run this command you will connect to a running container based on the `falconweb` image. You can navigate around `cd` and list files `ls` like a typical Linux environment. You can also start the application using `./PriceFalcon.Web` like our default entrypoint would do. When the application is running you can browse to http://localhost:6110 on your machine to connect to the server running in the container.

## GitHub actions for build

In keeping with doing this as cheaply as possible the GitHub ecosystem had the ability to run continuous integration (CI) and store Docker images in a [private container registry](https://docs.github.com/en/packages/guides/about-github-container-registry) for free at the time of writing.

Note that the GitHub Container Registry (`ghcr.io`) is in Beta and is different to the GitHub Packages Docker registry which is now deprecated. [You will need to enable the container registry](https://docs.github.com/en/packages/guides/enabling-improved-container-support#enabling-github-container-registry-for-your-personal-account) in your settings in order to use it. I spent a long time getting confused between GitHub's "Package Docker" and "Container" registry.

GitHub actions are the GitHub approach to CI. Each action is defined as a YAML (urgh, I hate this format!) file in the `.github/workflows` folder of your repository. For this project I defined 2 simple-ish workflows.

The first will build the code on a push in any branch. I don't have any tests worth running for this code so I just build the solution in this workflow:

```
name: Build

on: [push]

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@master

      - name: Set up dotnet core
        uses: actions/setup-dotnet@v1
        with:
          dotnet-version: "5.0.x"

      - name: Restore the solution
        run: dotnet restore src/PriceFalcon.sln

      - name: Build the solution
        run: dotnet build --no-restore -c Release src/PriceFalcon.sln
```

This should hopefully be fairly self-explanatory. It sets up .NET 5 on an Ubuntu VM and runs `dotnet restore` and `dotnet build` for the solution every time code is pushed to the repository.

Why no tests? For something like this unit testing isn't particularly valuable. I have about 1 place that there's some logic for which testing would be valuable. That's where I calculate whether an email should be sent for price changes or run failures per job. Otherwise the entire application is built on the interactions with external systems; SendGrid for emails, the browser for user flow, the database and Selenium/random web pages for the crawler.

I think the value of unit testing for C# is often incredibly overstated for these kinds of applications. The application at its current size is easy enough to manually test and if it grew enough to need tests I'd rather build end-to-end tests with Puppeteer or Selenium than unit tests. Everything else the compiler or manual testing will catch.

#### Build and deploy

A lot of the content of this workflow won't make much sense until our discussion of Kubernetes in the next post. I'll include it here since the next post will be long, but skip over it for now.

```
name: Docker

on:
  push:
    branches:
      - main
    tags:
      - v*
  pull_request:

env:
  WEB_IMAGE_NAME: falconweb
  CRAWL_IMAGE_NAME: falconcrawl

jobs:
  # Push image to GitHub Packages.
  push:
    runs-on: ubuntu-latest
    if: github.event_name == 'push'

    steps:
      - uses: actions/checkout@v2

      - name: Build web image
        run: pwd && docker build -f docker/web/Dockerfile -t $WEB_IMAGE_NAME .
        working-directory: .

      - name: Build crawler image
        run: docker build -f docker/crawler/Dockerfile -t $CRAWL_IMAGE_NAME .

      - name: Log into registry
        run: echo "${{ secrets.GITHUB_TOKEN }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin

      - name: Push image
        run: |
          WEB_IMAGE_ID=ghcr.io/eliotjones/$WEB_IMAGE_NAME
          CRAWL_IMAGE_ID=ghcr.io/eliotjones/$CRAWL_IMAGE_NAME

          # Use the commit SHA as the version.
          VERSION=${{ github.sha }}

          echo WEB_IMAGE_ID=$WEB_IMAGE_ID
          echo CRAWL_IMAGE_ID=$CRAWL_IMAGE_ID
          echo VERSION=$VERSION

          docker tag $WEB_IMAGE_NAME $WEB_IMAGE_ID:$VERSION
          docker push $WEB_IMAGE_ID:$VERSION

          docker tag $CRAWL_IMAGE_NAME $CRAWL_IMAGE_ID:$VERSION
          docker push $CRAWL_IMAGE_ID:$VERSION

      - name: Update deployment image id
        uses: datamonsters/replace-action@master
        with:
          files: "kubernetes/deployment.yaml"
          replacements: "ghcr.io/eliotjones/falconweb:latest=ghcr.io/eliotjones/falconweb:${{ github.sha }},ghcr.io/eliotjones/falconcrawl:latest=ghcr.io/eliotjones/falconcrawl:${{ github.sha }}"

      - name: Deploy using kubectl
        uses: steebchen/kubectl@master
        env:
          KUBE_CONFIG_DATA: ${{ secrets.KUBECONFIG }}
        with:
          args: apply -f kubernetes/deployment.yaml
```
